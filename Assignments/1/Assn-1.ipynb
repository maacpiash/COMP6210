{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP6210 Assignment 1\n",
    "\n",
    "## Part 2: Keyword extraction from tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name: Mohammad Abdul Ahad Chowdhury**\n",
    "    \n",
    "**ID: 46168249**\n",
    "\n",
    "**Email: mohammadabdul.ahadchowdhury@students.mq.edu.au**\n",
    "\n",
    "**Date: 17 March 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "First of all, we are going to clean up the text data. Then we will run lemmatization and removal of stopwords on the texts.\n",
    "\n",
    "We will be using [TF-IDF](https://en.wikipedia.org/wiki/Tfâ€“idf) method to extract keywords from these cleaned-up texts. The method consists of calculating Token Frequency (TF) for each text, and calculating Inverse Document Frequency (IDF) for each token of each text. Finally, we will use these two data structures to find out the keyword of each tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "We are going to use\n",
    "- **NLTK** for NLP tasks such as tokenization and lemmatization\n",
    "- **NumPy** for numeric operations\n",
    "- **Pandas** for two data structures: `DataFrame` and `Series`\n",
    "- **RE** for some operation regarding regular expression\n",
    "- **PyMongo** for connecting to MongoDB Atlas for CRUD operations\n",
    "\n",
    "Note: `pymongo` was chosen over `mongoengine` due to availability of documentation online at the time of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These downloads are needed to run some NLTK commands for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download ('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are reading the JSON file into a Pandas `DataFrame`. We're going to have two copies of the `DataFrame`: one to work with, and the other to have as a fresh reference copy, a source of truth, if things get messy with the other one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>place</th>\n",
       "      <th>entities</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5e6fcd2cfaabb5690b6eff12</td>\n",
       "      <td>Thu Apr 06 15:24:15 +0000 2017</td>\n",
       "      <td>1850006245121695744</td>\n",
       "      <td>Train people well enough so they can leave, tr...</td>\n",
       "      <td>{'id': 224499494501.0, 'name': 'user 01', 'scr...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'hashtags': [], 'urls': [{'url': 'twt url sam...</td>\n",
       "      <td>My number 1 Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e6fcd2cfaabb5690b6eff13</td>\n",
       "      <td>Thu Apr 06 15:24:16 +0000 2017</td>\n",
       "      <td>8150006245121695744</td>\n",
       "      <td>On Friday evening we welcomed our new MRes and...</td>\n",
       "      <td>{'id': 224499494502, 'name': 'user 02', 'scree...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'hashtags': [], 'urls': [{'url': 'twt url sam...</td>\n",
       "      <td>My number 2 Tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e6fcd2cfaabb5690b6eff14</td>\n",
       "      <td>Thu Apr 06 15:24:17 +0000 2017</td>\n",
       "      <td>8510006245121695744</td>\n",
       "      <td>Excited to publish the BPM Newsletter 1-2018, ...</td>\n",
       "      <td>{'id': 224499494503, 'name': 'user 03', 'scree...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'hashtags': [], 'urls': [{'url': 'twt url sam...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5e6fcd2cfaabb5690b6eff15</td>\n",
       "      <td>Thu Apr 06 15:24:18 +0000 2017</td>\n",
       "      <td>8501006245121695744</td>\n",
       "      <td>Our social networks  amplify negative more tha...</td>\n",
       "      <td>{'id': 224499494504, 'name': 'user 04', 'scree...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'hashtags': [], 'urls': [{'url': 'twt url sam...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5e6fcd2cfaabb5690b6eff16</td>\n",
       "      <td>Thu Apr 06 15:24:19 +0000 2017</td>\n",
       "      <td>8500106245121695744</td>\n",
       "      <td>Welcome to the official Department of Computin...</td>\n",
       "      <td>{'id': 224499494505, 'name': 'user 05', 'scree...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'hashtags': [], 'urls': [{'url': 'twt url sam...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id                      created_at  \\\n",
       "0  5e6fcd2cfaabb5690b6eff12  Thu Apr 06 15:24:15 +0000 2017   \n",
       "1  5e6fcd2cfaabb5690b6eff13  Thu Apr 06 15:24:16 +0000 2017   \n",
       "2  5e6fcd2cfaabb5690b6eff14  Thu Apr 06 15:24:17 +0000 2017   \n",
       "3  5e6fcd2cfaabb5690b6eff15  Thu Apr 06 15:24:18 +0000 2017   \n",
       "4  5e6fcd2cfaabb5690b6eff16  Thu Apr 06 15:24:19 +0000 2017   \n",
       "\n",
       "                id_str                                               text  \\\n",
       "0  1850006245121695744  Train people well enough so they can leave, tr...   \n",
       "1  8150006245121695744  On Friday evening we welcomed our new MRes and...   \n",
       "2  8510006245121695744  Excited to publish the BPM Newsletter 1-2018, ...   \n",
       "3  8501006245121695744  Our social networks  amplify negative more tha...   \n",
       "4  8500106245121695744  Welcome to the official Department of Computin...   \n",
       "\n",
       "                                                user place  \\\n",
       "0  {'id': 224499494501.0, 'name': 'user 01', 'scr...    {}   \n",
       "1  {'id': 224499494502, 'name': 'user 02', 'scree...    {}   \n",
       "2  {'id': 224499494503, 'name': 'user 03', 'scree...    {}   \n",
       "3  {'id': 224499494504, 'name': 'user 04', 'scree...    {}   \n",
       "4  {'id': 224499494505, 'name': 'user 05', 'scree...    {}   \n",
       "\n",
       "                                            entities                tag  \n",
       "0  {'hashtags': [], 'urls': [{'url': 'twt url sam...  My number 1 Tweet  \n",
       "1  {'hashtags': [], 'urls': [{'url': 'twt url sam...  My number 2 Tweet  \n",
       "2  {'hashtags': [], 'urls': [{'url': 'twt url sam...                NaN  \n",
       "3  {'hashtags': [], 'urls': [{'url': 'twt url sam...                NaN  \n",
       "4  {'hashtags': [], 'urls': [{'url': 'twt url sam...                NaN  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb+srv://maac:1234@cluster0-dtz35.mongodb.net/test?retryWrites=true&w=majority\")\n",
    "db = client.Tweets\n",
    "db_tweets = list(db.tweets.find())\n",
    "persistent_dataset = pd.DataFrame(db_tweets)\n",
    "# persistent_dataset is the original copy.\n",
    "dataset = persistent_dataset\n",
    "# The dataset we will work on\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "Let's say, we have an example tweet `I had a size 5 football.`\n",
    "\n",
    "For step 1, we will\n",
    "\n",
    "- convert all the text to lowercase: `i had a size 5 football.`\n",
    "- tokenize all the words: `[\"i\", \"had\", \"a\", \"size\", \"5\", \"football\", \".\"]`\n",
    "- get rid of all the numbers: `[\"i\", \"had\", \"a\", \"size\", \"football\", \".\"]`\n",
    "- remove all punctuation marks: `[\"i\", \"had\", \"a\", \"size\", \"football\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_tweets = []\n",
    "\n",
    "texts = dataset.text\n",
    "\n",
    "# step 1: convert whole text to lowercase\n",
    "tweets = texts.str.lower().to_list()\n",
    "\n",
    "for tweet in tweets:\n",
    "    # step 2: tokenize all the words\n",
    "    tokens = word_tokenize(tweet)\n",
    "\n",
    "    # step 3: remove all numeric tokens\n",
    "    re_num = re.compile(r'[0-9]')\n",
    "    tokens = [i for i in tokens if not re_num.match(i)]\n",
    "    \n",
    "    # step 4: remove all punctuation\n",
    "    re_pun = re.compile('[%s]' % re.escape(punctuation))\n",
    "    tokens = [i for i in tokens if not re_pun.match(i)]\n",
    "\n",
    "    tokenized_tweets.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use NLTK's `WordNetLemmatizer` to lemmatize the words. It means, all the words (i.e. 'running, 'ran') would be converted into their base forms (i.e. 'run'), so that the same words do not get counted more only because of a different form.\n",
    "\n",
    "So, our previous example now becomes like this: `[\"i\", \"have\", \"a\", \"size\", \"football\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_tweets = []\n",
    "\n",
    "for tweet in tokenized_tweets:\n",
    "    lemma = [wnl.lemmatize(token) for token in tweet]\n",
    "    lemmatized_tweets.append(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we remove the stopwords from the texts. Stopwords are some highly commonly used words (i.e. 'this', 'the') that do not contribute to the word count.\n",
    "\n",
    "So, our example becomes `[\"i\", \"have\", \"size\", \"football\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "filtered_tweets = []\n",
    "\n",
    "for tweet in lemmatized_tweets:\n",
    "    filtered = [token for token in tweet if token not in stop_words]\n",
    "    filtered_tweets.append(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation\n",
    "\n",
    "After clean-up, we move on to calculation. As mentioned before, to obtain TF-IDF we have to calculate `TF` and `IDF` first.\n",
    "\n",
    "`TF` or token frequency of a text is basically a table of how many times each word occurs in that text. Continuing from our example, here is the TF using `Counter` class from Python's own `collections` library:\n",
    "\n",
    "`Counter({'i': 1, 'have': 1, 'size': 1, 'football': 1})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tf = []\n",
    "\n",
    "for tweet in filtered_tweets:\n",
    "    tf.append(Counter(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate `IDF` or inverse document frequency for each token `T`, we use the following formula:\n",
    "\n",
    "`IDF(T) = log(number_of_documents Ã· number_of_documents_that_include_T )`\n",
    "\n",
    "This quantity signifies the amount of information a token provides across the texts it appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "total_tweets = len(filtered_tweets)\n",
    "idf = dict()\n",
    "\n",
    "for tweet in filtered_tweets:\n",
    "    for token in tweet:\n",
    "        if token not in idf.keys():\n",
    "            tweets_with_token = [idx for idx in range(total_tweets) if tf[idx][token] > 0]\n",
    "            occurences_of_token = len(tweets_with_token)\n",
    "            idf[token] = log(total_tweets / occurences_of_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, We calculate `TF-IDF` for each tweet and for each token. `TF-IDF` for a specific tweet and for a particular token is given by the product of the `TF` of the token for that tweet and the `IDF` of that token across the whole collection of texts. This process yields an `m`-by-`n` data structure (in this case, a `DataFrame`) where `n` is the number of tweets and `m` is the total number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = idf.keys()\n",
    "matrix = []\n",
    "\n",
    "for tweet_idx, tweet in enumerate(filtered_tweets):\n",
    "    numbers = []\n",
    "    for token in tokens:\n",
    "        numbers.append(tf[tweet_idx][token] * idf[token])\n",
    "    matrix.append(numbers)\n",
    "    \n",
    "tf_idf = pd.DataFrame(matrix, columns=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>people</th>\n",
       "      <th>well</th>\n",
       "      <th>enough</th>\n",
       "      <th>leave</th>\n",
       "      <th>treat</th>\n",
       "      <th>n't</th>\n",
       "      <th>want</th>\n",
       "      <th>â€”</th>\n",
       "      <th>richardbranson</th>\n",
       "      <th>...</th>\n",
       "      <th>master</th>\n",
       "      <th>career</th>\n",
       "      <th>universe</th>\n",
       "      <th>pretty</th>\n",
       "      <th>place</th>\n",
       "      <th>u</th>\n",
       "      <th>seems</th>\n",
       "      <th>awful</th>\n",
       "      <th>space</th>\n",
       "      <th>sagan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.912023</td>\n",
       "      <td>1.966113</td>\n",
       "      <td>7.824046</td>\n",
       "      <td>7.824046</td>\n",
       "      <td>3.912023</td>\n",
       "      <td>3.912023</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>3.912023</td>\n",
       "      <td>3.912023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 467 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      train    people      well    enough     leave     treat       n't  \\\n",
       "0  3.912023  1.966113  7.824046  7.824046  3.912023  3.912023  3.218876   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       want         â€”  richardbranson  ...  master  career  universe  pretty  \\\n",
       "0  3.218876  3.912023        3.912023  ...     0.0     0.0       0.0     0.0   \n",
       "1  0.000000  0.000000        0.000000  ...     0.0     0.0       0.0     0.0   \n",
       "2  0.000000  0.000000        0.000000  ...     0.0     0.0       0.0     0.0   \n",
       "3  0.000000  0.000000        0.000000  ...     0.0     0.0       0.0     0.0   \n",
       "4  0.000000  0.000000        0.000000  ...     0.0     0.0       0.0     0.0   \n",
       "\n",
       "   place    u  seems  awful  space  sagan  \n",
       "0    0.0  0.0    0.0    0.0    0.0    0.0  \n",
       "1    0.0  0.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0  0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.0  0.0    0.0    0.0    0.0    0.0  \n",
       "4    0.0  0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 467 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we sort this `DataFrame` column-wise by the highest value for each individual row, we can see the keyword for each tweet as a `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        well\n",
       "1     student\n",
       "2     excited\n",
       "3      social\n",
       "4    official\n",
       "dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = tf_idf.idxmax(axis=1)\n",
    "keywords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to CSV\n",
    "We export these values to a CSV file. `header` is set to `False` to get rid of one useless row at the top of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.to_csv('keywords.csv', header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the database\n",
    "We update the database with our newfound data using `pymongo` driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(db_tweets):\n",
    "    db.tweets.update_one(t, { \"$set\": { \"keyword\": keywords[i] } })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}